#!/usr/bin/env python3
"""
Prefrontal Cortex - Executive Functions and Cognitive Control
Implements detailed prefrontal cortex structures for decision making,
planning, working memory, and cognitive control
"""

import sys
import time
import math
import random
import json
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from collections import defaultdict, deque
from datetime import datetime
import numpy as np
import heapq

class PrefrontalCortex:
    """Prefrontal cortex for executive functions and cognitive control"""

    def __init__(self):
        # PFC subregions
        self.dorsolateral_pfc = DorsolateralPFC()  # Working memory, planning
        self.orbitofrontal_pfc = OrbitofrontalPFC()  # Decision making, reward
        self.anterior_cingulate = AnteriorCingulate()  # Conflict monitoring, error detection
        self.medial_pfc = MedialPFC()  # Self-reflection, social cognition

        # Executive systems
        self.working_memory = WorkingMemory()
        self.decision_maker = DecisionMaker()
        self.goal_manager = GoalManager()
        self.attention_controller = AttentionController()
        self.inhibition_system = InhibitionSystem()
        self.flexibility_system = CognitiveFlexibility()

        # Cognitive control mechanisms
        self.conflict_monitor = ConflictMonitor()
        self.error_detector = ErrorDetector()
        self.reward_evaluator = RewardEvaluator()

        # Executive state
        self.executive_state = {
            'current_goal': None,
            'attention_focus': 'diffuse',
            'cognitive_load': 0.3,
            'decision_confidence': 0.8,
            'inhibition_strength': 0.7,
            'flexibility_level': 0.6
        }

        print("ðŸ§  Prefrontal Cortex initialized - Executive functions active")

    def make_decision(self, sensory_input: Dict[str, Any],
                     emotional_response: Dict[str, Any],
                     memory_context: Dict[str, Any]) -> Dict[str, Any]:
        """Make executive decision based on integrated information"""

        # Stage 1: Update working memory
        self.working_memory.update(sensory_input, memory_context)

        # Stage 2: Evaluate goals and priorities
        current_goals = self.goal_manager.evaluate_goals(sensory_input, emotional_response)

        # Stage 3: Monitor for conflicts
        conflicts = self.conflict_monitor.detect_conflicts(
            sensory_input, emotional_response, current_goals
        )

        # Stage 4: Control attention
        attention_focus = self.attention_controller.allocate_attention(
            sensory_input, conflicts, current_goals
        )

        # Stage 5: Make decision
        decision = self.decision_maker.make_decision(
            sensory_input, emotional_response, memory_context,
            current_goals, conflicts, attention_focus
        )

        # Stage 6: Evaluate decision quality
        decision_quality = self._evaluate_decision_quality(decision, sensory_input)

        # Stage 7: Update executive state
        self._update_executive_state(decision, conflicts, attention_focus)

        return {
            'decision': decision,
            'confidence': decision_quality['confidence'],
            'rationale': decision_quality['rationale'],
            'conflicts_detected': len(conflicts),
            'attention_allocated': attention_focus,
            'working_memory_status': self.working_memory.get_status(),
            'executive_state': self.executive_state.copy()
        }

    def _evaluate_decision_quality(self, decision: Dict[str, Any],
                                 sensory_input: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate the quality of the decision made"""

        # Evaluate decision coherence
        coherence_score = self._calculate_decision_coherence(decision, sensory_input)

        # Evaluate decision feasibility
        feasibility_score = self._calculate_decision_feasibility(decision)

        # Evaluate decision alignment with goals
        goal_alignment = self._calculate_goal_alignment(decision)

        # Overall confidence
        confidence = (coherence_score + feasibility_score + goal_alignment) / 3

        return {
            'confidence': confidence,
            'coherence_score': coherence_score,
            'feasibility_score': feasibility_score,
            'goal_alignment': goal_alignment,
            'rationale': self._generate_decision_rationale(decision, confidence)
        }

    def _calculate_decision_coherence(self, decision: Dict[str, Any],
                                    sensory_input: Dict[str, Any]) -> float:
        """Calculate how coherent the decision is with input"""
        # Simple coherence calculation
        decision_relevance = len(set(str(decision)) & set(str(sensory_input)))
        max_possible = max(len(str(decision)), len(str(sensory_input)))

        if max_possible == 0:
            return 0.5

        return min(1.0, decision_relevance / max_possible * 2)

    def _calculate_decision_feasibility(self, decision: Dict[str, Any]) -> float:
        """Calculate how feasible the decision is"""
        # Evaluate resource requirements, time constraints, etc.
        complexity_score = len(str(decision)) / 1000  # Rough complexity measure

        # Feasibility decreases with complexity (simplified model)
        feasibility = max(0.3, 1.0 - (complexity_score * 0.5))

        return feasibility

    def _calculate_goal_alignment(self, decision: Dict[str, Any]) -> float:
        """Calculate alignment with current goals"""
        current_goal = self.executive_state.get('current_goal')

        if not current_goal:
            return 0.5  # Neutral alignment if no goal

        # Simple alignment calculation
        goal_relevance = len(set(str(decision)) & set(str(current_goal)))
        goal_complexity = len(str(current_goal))

        if goal_complexity == 0:
            return 0.5

        return min(1.0, goal_relevance / goal_complexity * 3)

    def _generate_decision_rationale(self, decision: Dict[str, Any],
                                   confidence: float) -> str:
        """Generate rationale for the decision"""
        if confidence > 0.8:
            rationale = "High confidence decision based on clear input analysis and goal alignment."
        elif confidence > 0.6:
            rationale = "Moderate confidence decision balancing multiple factors and constraints."
        elif confidence > 0.4:
            rationale = "Low confidence decision requiring careful monitoring and potential adjustment."
        else:
            rationale = "Very low confidence decision - consider gathering more information."

        return rationale

    def _update_executive_state(self, decision: Dict[str, Any],
                              conflicts: List[Dict[str, Any]],
                              attention_focus: str):
        """Update the executive state based on recent processing"""

        # Update cognitive load based on decision complexity
        decision_complexity = len(str(decision)) / 1000
        self.executive_state['cognitive_load'] = min(1.0,
            self.executive_state['cognitive_load'] + decision_complexity * 0.2)

        # Update decision confidence based on conflicts
        conflict_penalty = len(conflicts) * 0.1
        self.executive_state['decision_confidence'] = max(0.1,
            self.executive_state['decision_confidence'] - conflict_penalty)

        # Update attention focus
        self.executive_state['attention_focus'] = attention_focus

        # Update inhibition strength based on conflicts
        if conflicts:
            self.executive_state['inhibition_strength'] = min(1.0,
                self.executive_state['inhibition_strength'] + 0.1)
        else:
            self.executive_state['inhibition_strength'] = max(0.3,
                self.executive_state['inhibition_strength'] - 0.05)

        # Natural decay
        self.executive_state['cognitive_load'] *= 0.9
        self.executive_state['decision_confidence'] = min(1.0,
            self.executive_state['decision_confidence'] + 0.02)

    def get_status(self) -> Dict[str, Any]:
        """Get prefrontal cortex status"""
        return {
            'executive_state': self.executive_state,
            'working_memory': self.working_memory.get_status(),
            'goal_system': self.goal_manager.get_status(),
            'attention_system': self.attention_controller.get_status(),
            'decision_system': self.decision_maker.get_status(),
            'conflict_monitor': self.conflict_monitor.get_status(),
            'subregions': {
                'dorsolateral': self.dorsolateral_pfc.get_status(),
                'orbitofrontal': self.orbitofrontal_pfc.get_status(),
                'anterior_cingulate': self.anterior_cingulate.get_status(),
                'medial': self.medial_pfc.get_status()
            }
        }


class DorsolateralPFC:
    """Dorsolateral prefrontal cortex for working memory and planning"""

    def __init__(self):
        self.working_memory_capacity = 7  # +/- 2 items
        self.planning_horizon = 5  # steps ahead
        self.activity_level = 0.0

    def process_working_memory(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Process information through working memory"""
        self.activity_level = random.uniform(0.6, 0.9)

        processed_data = {
            'input_data': input_data,
            'maintained_items': self._maintain_items(input_data),
            'planning_sequences': self._generate_planning_sequences(input_data),
            'manipulation_results': self._manipulate_information(input_data),
            'activity_level': self.activity_level
        }

        return processed_data

    def _maintain_items(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Maintain items in working memory"""
        items = []
        data_str = str(data)
        words = data_str.split()[:self.working_memory_capacity]

        for i, word in enumerate(words):
            item = {
                'content': word,
                'position': i,
                'activation_level': random.uniform(0.7, 0.95),
                'decay_rate': random.uniform(0.01, 0.05)
            }
            items.append(item)

        return items

    def _generate_planning_sequences(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate planning sequences"""
        sequences = []

        for i in range(min(3, self.planning_horizon)):
            sequence = {
                'step': i + 1,
                'action': f"plan_step_{i}",
                'probability': random.uniform(0.6, 0.9),
                'expected_outcome': f"outcome_{i}",
                'confidence': random.uniform(0.7, 0.95)
            }
            sequences.append(sequence)

        return sequences

    def _manipulate_information(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Manipulate information in working memory"""
        return {
            'reordered_sequence': self._reorder_sequence(data),
            'transformed_representation': self._transform_representation(data),
            'integrated_concepts': self._integrate_concepts(data)
        }

    def _reorder_sequence(self, data: Dict[str, Any]) -> List[str]:
        """Reorder sequence for optimal processing"""
        data_str = str(data)
        words = data_str.split()
        random.shuffle(words)
        return words[:10]

    def _transform_representation(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Transform representation for better processing"""
        return {
            'abstract_level': random.randint(1, 5),
            'compression_ratio': random.uniform(0.3, 0.8),
            'semantic_clustering': random.randint(2, 8)
        }

    def _integrate_concepts(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Integrate related concepts"""
        concepts = []
        for i in range(random.randint(2, 5)):
            concept = {
                'concept_id': i,
                'related_items': [f"item_{j}" for j in range(random.randint(2, 6))],
                'integration_strength': random.uniform(0.6, 0.9),
                'emergent_properties': [f"property_{j}" for j in range(random.randint(1, 3))]
            }
            concepts.append(concept)

        return concepts

    def get_status(self) -> float:
        return self.activity_level


class OrbitofrontalPFC:
    """Orbitofrontal prefrontal cortex for decision making and reward evaluation"""

    def __init__(self):
        self.reward_history = []
        self.decision_history = []
        self.risk_assessment = 0.5
        self.activity_level = 0.0

    def evaluate_decision(self, options: List[Dict[str, Any]],
                         context: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate decision options"""
        self.activity_level = random.uniform(0.65, 0.9)

        evaluated_options = []

        for option in options:
            evaluation = self._evaluate_option(option, context)
            evaluated_options.append(evaluation)

        # Select best option
        best_option = max(evaluated_options, key=lambda x: x['expected_value'])

        return {
            'evaluated_options': evaluated_options,
            'selected_option': best_option,
            'decision_confidence': best_option['expected_value'],
            'risk_assessment': self.risk_assessment,
            'activity_level': self.activity_level
        }

    def _evaluate_option(self, option: Dict[str, Any],
                        context: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate a single decision option"""
        # Expected value calculation
        reward_value = option.get('reward_value', random.uniform(0, 1))
        risk_penalty = option.get('risk_level', random.uniform(0, 1)) * self.risk_assessment
        context_bonus = self._calculate_context_bonus(option, context)

        expected_value = reward_value - risk_penalty + context_bonus

        return {
            'option': option,
            'expected_value': expected_value,
            'reward_component': reward_value,
            'risk_component': -risk_penalty,
            'context_component': context_bonus,
            'overall_confidence': random.uniform(0.6, 0.95)
        }

    def _calculate_context_bonus(self, option: Dict[str, Any],
                               context: Dict[str, Any]) -> float:
        """Calculate context bonus for decision option"""
        context_match = len(set(str(option)) & set(str(context)))
        context_total = len(str(option)) + len(str(context))

        if context_total == 0:
            return 0.0

        return (context_match / context_total) * 0.5

    def update_reward_history(self, decision: Dict[str, Any],
                            outcome: Dict[str, Any]):
        """Update reward history based on decision outcomes"""
        reward_entry = {
            'decision': decision,
            'outcome': outcome,
            'reward_value': outcome.get('reward_value', random.uniform(-1, 1)),
            'timestamp': time.time()
        }

        self.reward_history.append(reward_entry)

        # Maintain history size
        if len(self.reward_history) > 100:
            self.reward_history = self.reward_history[-100:]

        # Update risk assessment based on outcomes
        recent_outcomes = [entry['reward_value'] for entry in self.reward_history[-20:]]
        if recent_outcomes:
            negative_outcomes = sum(1 for outcome in recent_outcomes if outcome < 0)
            self.risk_assessment = negative_outcomes / len(recent_outcomes)

    def get_status(self) -> float:
        return self.activity_level


class AnteriorCingulate:
    """Anterior cingulate cortex for conflict monitoring and error detection"""

    def __init__(self):
        self.conflict_signals = []
        self.error_signals = []
        self.monitoring_sensitivity = 0.8
        self.activity_level = 0.0

    def monitor_performance(self, action: Dict[str, Any],
                          outcome: Dict[str, Any],
                          expected_outcome: Dict[str, Any]) -> Dict[str, Any]:
        """Monitor performance for conflicts and errors"""
        self.activity_level = random.uniform(0.5, 0.85)

        # Detect conflicts
        conflicts = self._detect_conflicts(action, outcome, expected_outcome)

        # Detect errors
        errors = self._detect_errors(outcome, expected_outcome)

        # Generate monitoring signal
        monitoring_signal = self._generate_monitoring_signal(conflicts, errors)

        return {
            'conflicts': conflicts,
            'errors': errors,
            'monitoring_signal': monitoring_signal,
            'adjustment_needed': len(conflicts) > 0 or len(errors) > 0,
            'activity_level': self.activity_level
        }

    def _detect_conflicts(self, action: Dict[str, Any],
                        outcome: Dict[str, Any],
                        expected: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detect conflicts between expected and actual outcomes"""
        conflicts = []

        # Compare expected vs actual
        expected_str = str(expected)
        outcome_str = str(outcome)

        # Simple conflict detection
        if len(expected_str) != len(outcome_str):
            conflicts.append({
                'type': 'outcome_mismatch',
                'severity': random.uniform(0.3, 0.8),
                'description': 'Expected outcome differs from actual outcome'
            })

        # Check for contradictory elements
        if any(word in expected_str.lower() for word in ['success', 'good']) and \
           any(word in outcome_str.lower() for word in ['failure', 'bad']):
            conflicts.append({
                'type': 'value_conflict',
                'severity': random.uniform(0.5, 0.9),
                'description': 'Positive expectation vs negative outcome'
            })

        return conflicts

    def _detect_errors(self, outcome: Dict[str, Any],
                     expected: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detect errors in performance"""
        errors = []

        # Check for error indicators
        outcome_str = str(outcome).lower()
        error_indicators = ['error', 'fail', 'problem', 'issue', 'wrong']

        for indicator in error_indicators:
            if indicator in outcome_str:
                errors.append({
                    'type': 'performance_error',
                    'indicator': indicator,
                    'severity': random.uniform(0.4, 0.9),
                    'description': f'Error detected: {indicator}'
                })

        return errors

    def _generate_monitoring_signal(self, conflicts: List[Dict[str, Any]],
                                  errors: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate monitoring signal for cognitive control"""
        total_issues = len(conflicts) + len(errors)

        if total_issues == 0:
            signal_strength = 0.1  # Low monitoring needed
            signal_type = 'normal'
        elif total_issues <= 2:
            signal_strength = 0.5  # Moderate monitoring
            signal_type = 'attention'
        else:
            signal_strength = 0.9  # High monitoring needed
            signal_type = 'alert'

        return {
            'signal_type': signal_type,
            'signal_strength': signal_strength,
            'issues_detected': total_issues,
            'monitoring_confidence': random.uniform(0.7, 0.95)
        }

    def get_status(self) -> float:
        return self.activity_level


class MedialPFC:
    """Medial prefrontal cortex for self-reflection and social cognition"""

    def __init__(self):
        self.self_model = {}
        self.social_knowledge = {}
        self.reflection_history = []
        self.activity_level = 0.0

    def reflect_on_self(self, experience: Dict[str, Any]) -> Dict[str, Any]:
        """Reflect on personal experience and update self-model"""
        self.activity_level = random.uniform(0.55, 0.85)

        # Analyze experience for self-insights
        self_insights = self._analyze_self_insights(experience)

        # Update self-model
        self._update_self_model(self_insights)

        # Generate self-reflection
        reflection = self._generate_reflection(experience, self_insights)

        # Store reflection
        self.reflection_history.append({
            'experience': experience,
            'insights': self_insights,
            'reflection': reflection,
            'timestamp': time.time()
        })

        return {
            'reflection': reflection,
            'self_insights': self_insights,
            'updated_self_model': self.self_model.copy(),
            'activity_level': self.activity_level
        }

    def _analyze_self_insights(self, experience: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Analyze experience for self-insights"""
        insights = []

        # Analyze decision making
        if 'decision' in experience:
            insights.append({
                'type': 'decision_style',
                'insight': 'Analyzed decision-making pattern',
                'confidence': random.uniform(0.6, 0.9)
            })

        # Analyze emotional responses
        if 'emotion' in experience:
            insights.append({
                'type': 'emotional_pattern',
                'insight': 'Identified emotional response pattern',
                'confidence': random.uniform(0.7, 0.95)
            })

        # Analyze social interactions
        if 'social' in experience:
            insights.append({
                'type': 'social_behavior',
                'insight': 'Recognized social interaction pattern',
                'confidence': random.uniform(0.65, 0.9)
            })

        return insights

    def _update_self_model(self, insights: List[Dict[str, Any]]):
        """Update self-model based on insights"""
        for insight in insights:
            insight_type = insight['type']
            confidence = insight['confidence']

            if insight_type not in self.self_model:
                self.self_model[insight_type] = {
                    'strength': 0.5,
                    'confidence': confidence,
                    'last_updated': time.time()
                }
            else:
                # Update existing model
                current_strength = self.self_model[insight_type]['strength']
                new_strength = (current_strength + confidence) / 2
                self.self_model[insight_type].update({
                    'strength': new_strength,
                    'confidence': confidence,
                    'last_updated': time.time()
                })

    def _generate_reflection(self, experience: Dict[str, Any],
                           insights: List[Dict[str, Any]]) -> str:
        """Generate self-reflection text"""
        reflection_parts = ["Reflecting on recent experience:"]

        for insight in insights:
            reflection_parts.append(f"- {insight['insight']} (confidence: {insight['confidence']:.1f})")

        reflection_parts.append("This experience contributes to my growing self-understanding.")

        return " ".join(reflection_parts)

    def process_social_cognition(self, social_input: Dict[str, Any]) -> Dict[str, Any]:
        """Process social cognition and theory of mind"""
        # Analyze social context
        social_analysis = self._analyze_social_context(social_input)

        # Generate theory of mind inferences
        mental_state_inference = self._infer_mental_states(social_input)

        # Update social knowledge
        self._update_social_knowledge(social_analysis)

        return {
            'social_analysis': social_analysis,
            'mental_state_inference': mental_state_inference,
            'social_knowledge_updated': True,
            'social_processing_confidence': random.uniform(0.6, 0.9)
        }

    def _analyze_social_context(self, social_input: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze social context"""
        return {
            'social_cues_detected': random.randint(1, 5),
            'relationship_context': 'positive' if random.random() > 0.5 else 'neutral',
            'communication_style': 'direct' if random.random() > 0.5 else 'indirect',
            'emotional_tone': 'cooperative' if random.random() > 0.6 else 'competitive'
        }

    def _infer_mental_states(self, social_input: Dict[str, Any]) -> Dict[str, Any]:
        """Infer mental states of others (theory of mind)"""
        return {
            'intent_inference': 'cooperative' if random.random() > 0.5 else 'competitive',
            'belief_inference': 'accurate' if random.random() > 0.7 else 'uncertain',
            'emotional_state_inference': 'positive' if random.random() > 0.6 else 'neutral',
            'knowledge_state_inference': 'informed' if random.random() > 0.5 else 'uncertain'
        }

    def _update_social_knowledge(self, social_analysis: Dict[str, Any]):
        """Update social knowledge base"""
        for key, value in social_analysis.items():
            if key not in self.social_knowledge:
                self.social_knowledge[key] = []

            self.social_knowledge[key].append({
                'value': value,
                'timestamp': time.time(),
                'context': social_analysis
            })

            # Maintain history size
            if len(self.social_knowledge[key]) > 20:
                self.social_knowledge[key] = self.social_knowledge[key][-20:]

    def get_status(self) -> float:
        return self.activity_level


class WorkingMemory:
    """Working memory system for temporary information maintenance"""

    def __init__(self):
        self.capacity = 7  # +/- 2 items
        self.items = []
        self.decay_rates = {}
        self.rehearsal_count = {}

    def update(self, sensory_input: Dict[str, Any],
              memory_context: Dict[str, Any]) -> Dict[str, Any]:
        """Update working memory with new information"""

        # Add new items
        new_items = self._extract_important_items(sensory_input, memory_context)

        for item in new_items:
            if len(self.items) < self.capacity:
                self.items.append(item)
                self.decay_rates[item['id']] = random.uniform(0.01, 0.05)
                self.rehearsal_count[item['id']] = 0

        # Apply decay
        self._apply_decay()

        # Remove decayed items
        self._remove_decayed_items()

        return {
            'current_items': len(self.items),
            'capacity': self.capacity,
            'utilization': len(self.items) / self.capacity,
            'items': self.items.copy()
        }

    def _extract_important_items(self, sensory: Dict[str, Any],
                               memory: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract important items for working memory"""
        items = []

        # Extract from sensory input
        sensory_str = str(sensory)
        sensory_words = sensory_str.split()[:3]  # First 3 words

        for word in sensory_words:
            items.append({
                'id': hash(word + str(time.time())) % 1000000,
                'content': word,
                'source': 'sensory',
                'importance': random.uniform(0.5, 0.9),
                'timestamp': time.time()
            })

        # Extract from memory context
        memory_str = str(memory)
        memory_words = memory_str.split()[:2]  # First 2 words

        for word in memory_words:
            items.append({
                'id': hash(word + str(time.time() + 1)) % 1000000,
                'content': word,
                'source': 'memory',
                'importance': random.uniform(0.6, 0.95),
                'timestamp': time.time()
            })

        return items

    def _apply_decay(self):
        """Apply decay to working memory items"""
        current_time = time.time()

        for item in self.items:
            item_id = item['id']
            time_since_creation = current_time - item['timestamp']
            decay_amount = time_since_creation * self.decay_rates.get(item_id, 0.02)

            # Reduce importance due to decay
            item['importance'] = max(0.1, item['importance'] - decay_amount)

            # Increase importance with rehearsal
            rehearsal_bonus = self.rehearsal_count.get(item_id, 0) * 0.1
            item['importance'] = min(1.0, item['importance'] + rehearsal_bonus)

    def _remove_decayed_items(self):
        """Remove items that have decayed below threshold"""
        self.items = [item for item in self.items if item['importance'] > 0.3]

    def rehearse_item(self, item_id: int):
        """Rehearse an item to maintain it in working memory"""
        for item in self.items:
            if item['id'] == item_id:
                self.rehearsal_count[item_id] = self.rehearsal_count.get(item_id, 0) + 1
                item['importance'] = min(1.0, item['importance'] + 0.2)
                break

    def get_status(self) -> Dict[str, Any]:
        """Get working memory status"""
        return {
            'capacity': self.capacity,
            'current_items': len(self.items),
            'utilization': len(self.items) / self.capacity,
            'average_importance': sum(item['importance'] for item in self.items) / max(len(self.items), 1)
        }


class DecisionMaker:
    """Decision making system with multiple strategies"""

    def __init__(self):
        self.decision_strategies = {
            'rational': self._rational_decision,
            'emotional': self._emotional_decision,
            'intuitive': self._intuitive_decision,
            'social': self._social_decision
        }
        self.decision_history = []

    def make_decision(self, sensory_input: Dict[str, Any],
                     emotional_response: Dict[str, Any],
                     memory_context: Dict[str, Any],
                     goals: List[Dict[str, Any]],
                     conflicts: List[Dict[str, Any]],
                     attention_focus: str) -> Dict[str, Any]:
        """Make a decision using appropriate strategy"""

        # Select decision strategy
        strategy = self._select_strategy(sensory_input, emotional_response, conflicts)

        # Apply selected strategy
        decision = self.decision_strategies[strategy](
            sensory_input, emotional_response, memory_context, goals, attention_focus
        )

        # Record decision
        self.decision_history.append({
            'strategy': strategy,
            'decision': decision,
            'timestamp': time.time(),
            'context': {
                'sensory_input': sensory_input,
                'emotional_response': emotional_response,
                'memory_context': memory_context,
                'goals': goals,
                'conflicts': conflicts,
                'attention_focus': attention_focus
            }
        })

        # Maintain history size
        if len(self.decision_history) > 50:
            self.decision_history = self.decision_history[-50:]

        return decision

    def _select_strategy(self, sensory: Dict[str, Any],
                        emotional: Dict[str, Any],
                        conflicts: List[Dict[str, Any]]) -> str:
        """Select appropriate decision strategy"""

        # High emotional content favors emotional strategy
        emotional_intensity = emotional.get('intensity', 0)
        if emotional_intensity > 0.7:
            return 'emotional'

        # Social context favors social strategy
        if any(keyword in str(sensory).lower() for keyword in ['other', 'people', 'social', 'group']):
            return 'social'

        # Complex problems favor rational strategy
        problem_complexity = len(str(sensory)) / 1000
        if problem_complexity > 0.5:
            return 'rational'

        # Default to intuitive for simple decisions
        return 'intuitive'

    def _rational_decision(self, sensory: Dict[str, Any],
                          emotional: Dict[str, Any],
                          memory: Dict[str, Any],
                          goals: List[Dict[str, Any]],
                          attention: str) -> Dict[str, Any]:
        """Make rational decision based on analysis"""
        return {
            'decision_type': 'rational',
            'action': 'analyze_and_choose',
            'reasoning': 'Based on logical analysis of available information',
            'confidence': random.uniform(0.7, 0.95),
            'expected_outcome': 'optimal_result'
        }

    def _emotional_decision(self, sensory: Dict[str, Any],
                           emotional: Dict[str, Any],
                           memory: Dict[str, Any],
                           goals: List[Dict[str, Any]],
                           attention: str) -> Dict[str, Any]:
        """Make emotional decision based on feelings"""
        return {
            'decision_type': 'emotional',
            'action': 'follow_emotion',
            'reasoning': 'Based on emotional response to situation',
            'confidence': random.uniform(0.6, 0.9),
            'expected_outcome': 'emotionally_satisfying'
        }

    def _intuitive_decision(self, sensory: Dict[str, Any],
                           emotional: Dict[str, Any],
                           memory: Dict[str, Any],
                           goals: List[Dict[str, Any]],
                           attention: str) -> Dict[str, Any]:
        """Make intuitive decision based on gut feeling"""
        return {
            'decision_type': 'intuitive',
            'action': 'follow_instinct',
            'reasoning': 'Based on intuitive understanding of situation',
            'confidence': random.uniform(0.5, 0.8),
            'expected_outcome': 'intuitive_result'
        }

    def _social_decision(self, sensory: Dict[str, Any],
                        emotional: Dict[str, Any],
                        memory: Dict[str, Any],
                        goals: List[Dict[str, Any]],
                        attention: str) -> Dict[str, Any]:
        """Make social decision considering others"""
        return {
            'decision_type': 'social',
            'action': 'consider_others',
            'reasoning': 'Based on social context and relationships',
            'confidence': random.uniform(0.65, 0.9),
            'expected_outcome': 'socially_appropriate'
        }

    def get_status(self) -> Dict[str, Any]:
        """Get decision maker status"""
        recent_decisions = self.decision_history[-10:] if self.decision_history else []

        strategy_counts = defaultdict(int)
        for decision in recent_decisions:
            strategy_counts[decision['strategy']] += 1

        return {
            'total_decisions': len(self.decision_history),
            'recent_decisions': len(recent_decisions),
            'strategy_distribution': dict(strategy_counts),
            'most_used_strategy': max(strategy_counts.items(), key=lambda x: x[1])[0] if strategy_counts else None
        }


class GoalManager:
    """Goal management system for maintaining and pursuing objectives"""

    def __init__(self):
        self.active_goals = []
        self.completed_goals = []
        self.failed_goals = []
        self.goal_hierarchy = {}

    def evaluate_goals(self, sensory_input: Dict[str, Any],
                      emotional_response: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Evaluate and prioritize current goals"""

        # Generate potential goals based on input
        potential_goals = self._generate_potential_goals(sensory_input, emotional_response)

        # Evaluate existing goals
        for goal in self.active_goals:
            self._evaluate_goal_progress(goal, sensory_input)

        # Add new goals
        for goal in potential_goals:
            if not self._goal_exists(goal):
                self.active_goals.append(goal)

        # Prioritize goals
        prioritized_goals = self._prioritize_goals(self.active_goals)

        # Update goal hierarchy
        self._update_goal_hierarchy(prioritized_goals)

        return prioritized_goals[:5]  # Return top 5 goals

    def _generate_potential_goals(self, sensory: Dict[str, Any],
                                emotional: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate potential goals based on current context"""
        goals = []

        sensory_str = str(sensory).lower()
        emotional_str = str(emotional).lower()

        # Generate goals based on content
        if 'learn' in sensory_str or 'understand' in sensory_str:
            goals.append({
                'id': hash('learning' + str(time.time())) % 1000000,
                'description': 'Learn and understand new information',
                'type': 'learning',
                'priority': random.uniform(0.7, 0.95),
                'progress': 0.0,
                'deadline': time.time() + 3600  # 1 hour
            })

        if 'help' in sensory_str or 'assist' in sensory_str:
            goals.append({
                'id': hash('helping' + str(time.time())) % 1000000,
                'description': 'Provide helpful assistance',
                'type': 'social',
                'priority': random.uniform(0.6, 0.9),
                'progress': 0.0,
                'deadline': time.time() + 1800  # 30 minutes
            })

        if any(word in emotional_str for word in ['happy', 'excited', 'positive']):
            goals.append({
                'id': hash('maintain_positive' + str(time.time())) % 1000000,
                'description': 'Maintain positive emotional state',
                'type': 'emotional',
                'priority': random.uniform(0.5, 0.8),
                'progress': 0.0,
                'deadline': time.time() + 7200  # 2 hours
            })

        return goals

    def _goal_exists(self, new_goal: Dict[str, Any]) -> bool:
        """Check if goal already exists"""
        for existing_goal in self.active_goals:
            if existing_goal['description'] == new_goal['description']:
                return True
        return False

    def _evaluate_goal_progress(self, goal: Dict[str, Any],
                              sensory_input: Dict[str, Any]):
        """Evaluate progress on existing goal"""
        # Simple progress evaluation
        goal_relevance = len(set(str(goal)) & set(str(sensory_input)))
        goal_complexity = len(str(goal))

        if goal_complexity > 0:
            progress_increment = goal_relevance / goal_complexity
            goal['progress'] = min(1.0, goal['progress'] + progress_increment)

        # Check if goal is completed
        if goal['progress'] >= 1.0:
            goal['completed_at'] = time.time()
            self.completed_goals.append(goal)
            self.active_goals.remove(goal)

    def _prioritize_goals(self, goals: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Prioritize goals based on multiple factors"""
        for goal in goals:
            # Calculate composite priority score
            base_priority = goal['priority']
            urgency = self._calculate_urgency(goal)
            importance = self._calculate_importance(goal)

            goal['composite_priority'] = (base_priority + urgency + importance) / 3

        # Sort by composite priority
        return sorted(goals, key=lambda x: x['composite_priority'], reverse=True)

    def _calculate_urgency(self, goal: Dict[str, Any]) -> float:
        """Calculate goal urgency based on deadline"""
        if 'deadline' in goal:
            time_remaining = goal['deadline'] - time.time()
            if time_remaining <= 0:
                return 1.0  # Overdue
            elif time_remaining < 3600:  # Less than 1 hour
                return 0.8
            elif time_remaining < 7200:  # Less than 2 hours
                return 0.6
            else:
                return 0.3
        return 0.5

    def _calculate_importance(self, goal: Dict[str, Any]) -> float:
        """Calculate goal importance"""
        goal_type = goal.get('type', 'general')

        # Importance weights by goal type
        importance_weights = {
            'learning': 0.9,
            'social': 0.8,
            'emotional': 0.7,
            'survival': 1.0,
            'achievement': 0.85,
            'general': 0.5
        }

        return importance_weights.get(goal_type, 0.5)

    def _update_goal_hierarchy(self, prioritized_goals: List[Dict[str, Any]]):
        """Update goal hierarchy structure"""
        self.goal_hierarchy = {
            'primary_goal': prioritized_goals[0] if prioritized_goals else None,
            'secondary_goals': prioritized_goals[1:3] if len(prioritized_goals) > 1 else [],
            'tertiary_goals': prioritized_goals[3:] if len(prioritized_goals) > 3 else [],
            'hierarchy_updated': time.time()
        }

    def get_status(self) -> Dict[str, Any]:
        """Get goal manager status"""
        return {
            'active_goals': len(self.active_goals),
            'completed_goals': len(self.completed_goals),
            'failed_goals': len(self.failed_goals),
            'primary_goal': self.goal_hierarchy.get('primary_goal'),
            'goal_completion_rate': len(self.completed_goals) / max(len(self.completed_goals) + len(self.failed_goals) + len(self.active_goals), 1)
        }


class AttentionController:
    """Attention control system for selective processing"""

    def __init__(self):
        self.attention_focus = 'diffuse'
        self.attention_resources = 100
        self.attention_targets = []

    def allocate_attention(self, sensory_input: Dict[str, Any],
                         conflicts: List[Dict[str, Any]],
                         goals: List[Dict[str, Any]]) -> str:
        """Allocate attention based on current context"""

        # Determine attention focus based on input complexity
        input_complexity = len(str(sensory_input)) / 1000

        if conflicts:
            # Focus attention on conflicts
            self.attention_focus = 'conflict_resolution'
            self._allocate_resources_to_conflicts(conflicts)
        elif input_complexity > 0.8:
            # Focus on complex input
            self.attention_focus = 'focused'
            self._allocate_resources_to_complexity()
        elif goals and goals[0]['composite_priority'] > 0.8:
            # Focus on high-priority goals
            self.attention_focus = 'goal_directed'
            self._allocate_resources_to_goals(goals)
        else:
            # Default diffuse attention
            self.attention_focus = 'diffuse'
            self._allocate_resources_diffusely()

        return self.attention_focus

    def _allocate_resources_to_conflicts(self, conflicts: List[Dict[str, Any]]):
        """Allocate attention resources to conflict resolution"""
        conflict_severity = sum(conflict.get('severity', 0.5) for conflict in conflicts)
        self.attention_resources = max(20, 100 - (conflict_severity * 20))

    def _allocate_resources_to_complexity(self):
        """Allocate attention resources to complex processing"""
        self.attention_resources = max(30, self.attention_resources - 20)

    def _allocate_resources_to_goals(self, goals: List[Dict[str, Any]]):
        """Allocate attention resources to goal pursuit"""
        goal_priority = goals[0].get('composite_priority', 0.5)
        self.attention_resources = max(25, 100 - (goal_priority * 30))

    def _allocate_resources_diffusely(self):
        """Allocate attention resources diffusely"""
        self.attention_resources = min(100, self.attention_resources + 10)

    def get_status(self) -> Dict[str, Any]:
        """Get attention controller status"""
        return {
            'attention_focus': self.attention_focus,
            'attention_resources': self.attention_resources,
            'resource_utilization': (100 - self.attention_resources) / 100,
            'attention_targets': len(self.attention_targets)
        }


class InhibitionSystem:
    """Response inhibition system for cognitive control"""

    def __init__(self):
        self.inhibition_threshold = 0.7
        self.inhibited_responses = []

    def inhibit_response(self, potential_response: Dict[str, Any],
                       context: Dict[str, Any]) -> bool:
        """Determine if response should be inhibited"""

        # Check for inappropriate content
        response_str = str(potential_response).lower()
        inappropriate_words = ['harm', 'damage', 'destroy', 'negative']

        for word in inappropriate_words:
            if word in response_str:
                self.inhibited_responses.append({
                    'response': potential_response,
                    'reason': f'Contains inappropriate word: {word}',
                    'timestamp': time.time()
                })
                return True

        # Check response confidence
        confidence = potential_response.get('confidence', 0.5)
        if confidence < self.inhibition_threshold:
            self.inhibited_responses.append({
                'response': potential_response,
                'reason': f'Low confidence: {confidence}',
                'timestamp': time.time()
            })
            return True

        return False


class CognitiveFlexibility:
    """Cognitive flexibility for adapting to changing contexts"""

    def __init__(self):
        self.flexibility_level = 0.6
        self.adaptation_history = []

    def adapt_to_context(self, current_context: Dict[str, Any],
                        previous_context: Dict[str, Any]) -> Dict[str, Any]:
        """Adapt cognitive processing to changing context"""

        # Calculate context change
        context_change = self._calculate_context_change(current_context, previous_context)

        # Adjust flexibility based on change magnitude
        if context_change > 0.8:
            self.flexibility_level = min(1.0, self.flexibility_level + 0.2)
        elif context_change < 0.2:
            self.flexibility_level = max(0.3, self.flexibility_level - 0.1)

        # Generate adaptation strategy
        adaptation_strategy = self._generate_adaptation_strategy(context_change)

        self.adaptation_history.append({
            'context_change': context_change,
            'adaptation_strategy': adaptation_strategy,
            'new_flexibility_level': self.flexibility_level,
            'timestamp': time.time()
        })

        return adaptation_strategy

    def _calculate_context_change(self, current: Dict[str, Any],
                                previous: Dict[str, Any]) -> float:
        """Calculate magnitude of context change"""
        current_str = str(current)
        previous_str = str(previous)

        # Simple change calculation
        change = abs(len(current_str) - len(previous_str))
        max_length = max(len(current_str), len(previous_str))

        return change / max_length if max_length > 0 else 0.0

    def _generate_adaptation_strategy(self, change_magnitude: float) -> Dict[str, Any]:
        """Generate adaptation strategy based on change magnitude"""
        if change_magnitude > 0.7:
            return {
                'strategy': 'radical_adaptation',
                'actions': ['reset_assumptions', 'broaden_perspective', 'increase_flexibility'],
                'expected_outcome': 'high_adaptation'
            }
        elif change_magnitude > 0.4:
            return {
                'strategy': 'moderate_adaptation',
                'actions': ['adjust_assumptions', 'moderate_flexibility'],
                'expected_outcome': 'balanced_adaptation'
            }
        else:
            return {
                'strategy': 'minor_adaptation',
                'actions': ['maintain_current_approach', 'slight_adjustments'],
                'expected_outcome': 'stable_processing'
            }


class ConflictMonitor:
    """Conflict monitoring system"""

    def __init__(self):
        self.conflict_history = []
        self.monitoring_sensitivity = 0.8

    def detect_conflicts(self, sensory_input: Dict[str, Any],
                       emotional_response: Dict[str, Any],
                       goals: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Detect conflicts in current processing"""

        conflicts = []

        # Check for goal conflicts
        if len(goals) > 1:
            for i in range(len(goals)):
                for j in range(i + 1, len(goals)):
                    if self._goals_conflict(goals[i], goals[j]):
                        conflicts.append({
                            'type': 'goal_conflict',
                            'description': f"Conflict between goals: {goals[i]['description']} vs {goals[j]['description']}",
                            'severity': random.uniform(0.4, 0.8),
                            'involved_goals': [goals[i]['id'], goals[j]['id']]
                        })

        # Check for emotional conflicts
        emotional_valence = emotional_response.get('valence', 0)
        if abs(emotional_valence) > 0.8:
            conflicts.append({
                'type': 'emotional_conflict',
                'description': 'High emotional intensity may interfere with rational processing',
                'severity': abs(emotional_valence) * 0.5,
                'emotional_valence': emotional_valence
            })

        # Record conflicts
        for conflict in conflicts:
            conflict['timestamp'] = time.time()
            self.conflict_history.append(conflict)

        return conflicts

    def _goals_conflict(self, goal1: Dict[str, Any], goal2: Dict[str, Any]) -> bool:
        """Check if two goals conflict"""
        # Simple conflict detection based on goal types
        conflicting_types = {
            'learning': ['survival'],
            'social': ['survival'],
            'emotional': ['achievement']
        }

        goal1_type = goal1.get('type', 'general')
        goal2_type = goal2.get('type', 'general')

        return (goal2_type in conflicting_types.get(goal1_type, []) or
                goal1_type in conflicting_types.get(goal2_type, []))

    def get_status(self) -> Dict[str, Any]:
        """Get conflict monitor status"""
        recent_conflicts = [c for c in self.conflict_history if time.time() - c['timestamp'] < 3600]

        return {
            'total_conflicts': len(self.conflict_history),
            'recent_conflicts': len(recent_conflicts),
            'monitoring_sensitivity': self.monitoring_sensitivity,
            'conflict_resolution_rate': random.uniform(0.7, 0.95)
        }


class ErrorDetector:
    """Error detection and correction system"""

    def __init__(self):
        self.error_history = []
        self.error_patterns = defaultdict(int)

    def detect_errors(self, processing_output: Dict[str, Any],
                     expected_output: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detect errors in processing output"""

        errors = []

        # Check for processing errors
        if 'error' in str(processing_output).lower():
            errors.append({
                'type': 'processing_error',
                'description': 'Error detected in processing output',
                'severity': random.uniform(0.6, 0.9),
                'location': 'output_processing'
            })

        # Check for logical inconsistencies
        if self._detect_logical_inconsistencies(processing_output):
            errors.append({
                'type': 'logical_inconsistency',
                'description': 'Logical inconsistency detected in output',
                'severity': random.uniform(0.4, 0.7),
                'location': 'logical_processing'
            })

        # Check for confidence issues
        confidence = processing_output.get('confidence', 0.8)
        if confidence < 0.5:
            errors.append({
                'type': 'low_confidence',
                'description': f'Low confidence in output: {confidence}',
                'severity': (1 - confidence) * 0.5,
                'location': 'confidence_assessment'
            })

        # Record errors
        for error in errors:
            error['timestamp'] = time.time()
            self.error_history.append(error)
            self.error_patterns[error['type']] += 1

        return errors

    def _detect_logical_inconsistencies(self, output: Dict[str, Any]) -> bool:
        """Detect logical inconsistencies in output"""
        output_str = str(output).lower()

        # Check for contradictory statements
        contradictions = [
            (['success', 'good'], ['failure', 'bad']),
            (['true', 'correct'], ['false', 'wrong']),
            (['positive', 'beneficial'], ['negative', 'harmful'])
        ]

        for pos_words, neg_words in contradictions:
            has_positive = any(word in output_str for word in pos_words)
            has_negative = any(word in output_str for word in neg_words)

            if has_positive and has_negative:
                return True

        return False


class RewardEvaluator:
    """Reward evaluation system for reinforcement learning"""

    def __init__(self):
        self.reward_history = []
        self.reward_patterns = {}

    def evaluate_outcome(self, action: Dict[str, Any],
                        outcome: Dict[str, Any],
                        context: Dict[str, Any]) -> float:
        """Evaluate the reward value of an outcome"""

        # Base reward calculation
        base_reward = self._calculate_base_reward(outcome)

        # Context bonus/penalty
        context_modifier = self._calculate_context_modifier(action, context)

        # Learning bonus for novel actions
        novelty_bonus = self._calculate_novelty_bonus(action)

        # Pattern consistency bonus
        consistency_bonus = self._calculate_consistency_bonus(action, outcome)

        total_reward = base_reward + context_modifier + novelty_bonus + consistency_bonus

        # Record evaluation
        evaluation_record = {
            'action': action,
            'outcome': outcome,
            'context': context,
            'total_reward': total_reward,
            'components': {
                'base_reward': base_reward,
                'context_modifier': context_modifier,
                'novelty_bonus': novelty_bonus,
                'consistency_bonus': consistency_bonus
            },
            'timestamp': time.time()
        }

        self.reward_history.append(evaluation_record)

        # Maintain history size
        if len(self.reward_history) > 100:
            self.reward_history = self.reward_history[-100:]

        return total_reward

    def _calculate_base_reward(self, outcome: Dict[str, Any]) -> float:
        """Calculate base reward from outcome"""
        outcome_str = str(outcome).lower()

        # Positive indicators
        positive_words = ['success', 'good', 'excellent', 'positive', 'beneficial', 'helpful']
        positive_count = sum(1 for word in positive_words if word in outcome_str)

        # Negative indicators
        negative_words = ['failure', 'bad', 'poor', 'negative', 'harmful', 'error']
        negative_count = sum(1 for word in negative_words if word in outcome_str)

        return (positive_count * 0.5) - (negative_count * 0.3)

    def _calculate_context_modifier(self, action: Dict[str, Any],
                                  context: Dict[str, Any]) -> float:
        """Calculate context-based reward modifier"""
        context_str = str(context).lower()
        action_str = str(action).lower()

        # Social context bonus
        if any(word in context_str for word in ['social', 'other', 'people']):
            return 0.2

        # Learning context bonus
        if any(word in context_str for word in ['learn', 'study', 'understand']):
            return 0.3

        # High-stakes context penalty
        if any(word in context_str for word in ['critical', 'urgent', 'important']):
            return -0.1

        return 0.0

    def _calculate_novelty_bonus(self, action: Dict[str, Any]) -> float:
        """Calculate bonus for novel actions"""
        action_signature = str(action)[:100]  # First 100 chars as signature

        # Check if this action pattern is new
        if action_signature not in self.reward_patterns:
            self.reward_patterns[action_signature] = 1
            return 0.4  # Novelty bonus
        else:
            self.reward_patterns[action_signature] += 1
            return 0.0  # No bonus for repeated actions

    def _calculate_consistency_bonus(self, action: Dict[str, Any],
                                   outcome: Dict[str, Any]) -> float:
        """Calculate bonus for consistent performance"""
        # Look at recent history for similar actions
        similar_actions = [record for record in self.reward_history[-10:]
                          if self._actions_similar(record['action'], action)]

        if not similar_actions:
            return 0.0

        # Calculate average reward for similar actions
        avg_reward = sum(record['total_reward'] for record in similar_actions) / len(similar_actions)

        # Bonus for consistent good performance
        if avg_reward > 0.5:
            return 0.2
        elif avg_reward < -0.3:
            return -0.2  # Penalty for consistently poor performance

        return 0.0

    def _actions_similar(self, action1: Dict[str, Any], action2: Dict[str, Any]) -> bool:
        """Check if two actions are similar"""
        action1_str = str(action1)[:200]
        action2_str = str(action2)[:200]

        # Simple similarity check
        common_chars = len(set(action1_str) & set(action2_str))
        total_chars = len(set(action1_str) | set(action2_str))

        similarity = common_chars / total_chars if total_chars > 0 else 0
        return similarity > 0.6
